var documenterSearchIndex = {"docs":
[{"location":"guide.html#Guide","page":"Guide","title":"Guide","text":"","category":"section"},{"location":"guide.html#Derivatives-(Dense-Jacobians)","page":"Guide","title":"Derivatives (Dense Jacobians)","text":"","category":"section"},{"location":"guide.html","page":"Guide","title":"Guide","text":"SNOW wraps multiple packages to make it easier to efficiently compute derivatives. For dense Jacobians switching between derivative methods is straightforward and just requires selecting different options (and ensuring that your code is compatible with the method of choice).","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"Caches are created pre-optimization so that zero (or near zero) allocations occur during optimization.  If you wish to fully take advantage of this speed, your own code should also not allocate.  We also reuse outputs where possible to avoid additional unnecessary function calls.","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"We use FiniteDiff.jl for finite differencing and complex step, ForwardDiff.jl for forward-mode AD, and ReverseDiff.jl for reverse-mode AD.  There is some limited support of Zygote.jl (just gradients not Jacobians currently) although it requires that your code does not mutate arrays.","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"using SNOW\n\n# forward finite difference\noptions = Options(derivatives=ForwardFD())\n\n# central finite difference\noptions = Options(derivatives=CentralFD())\n\n# complex step\noptions = Options(derivatives=ComplexStep())\n\n# forward-mode algorithmic differentiation\noptions = Options(derivatives=ForwardAD())\n\n# reverse-mode algorithmic differentiation\noptions = Options(derivatives=ReverseAD())\n\n# Zygote (reverse-mode source-to-source AD)\noptions = Options(derivatives=RevZyg())\n\nnothing #hide","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"Note that dense Jacobians are the default so the above are equivalent to explicitly specifying dense:","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"options = Options(derivatives=ForwardAD(), sparsity=DensePattern())\nnothing #hide","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"You can also specify your own derivatives, by setting this option:","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"options = Options(derivatives=UserDeriv())\nnothing #hide","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"The function signature you provide should also be modified as follows:","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"f = func!(g, df, dg, x)","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"with the two new inputs: df and dg that should be modified in place.  df is a vector containing the objective gradient d f  dx_j and  dg is a matrix containing the constraint jacobian dg_i  dx_j. Below is a simple example:","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"using SNOW\n\nfunction deriv(g, df, dg, x)\n\n    # objective\n    f = x[1]^2 - 0.5*x[1] - x[2] - 2\n\n    # constraints\n    g[1] = x[1]^2 - 4*x[1] + x[2] + 1\n    g[2] = 0.5*x[1]^2 + x[2]^2 - x[1] - 4\n\n    # gradient\n    df[1] = 2*x[1] - 0.5\n    df[2] = -1\n\n    # jacobian\n    dg[1, 1] = 2*x[1] - 4\n    dg[1, 2] = 1\n    dg[2, 1] = x[1] - 1\n    dg[2, 2] = 2*x[2]\n\nend\n\nx0 = [1.0; 1.0]  # starting point\nlx = [-5.0, -5]  # lower bounds on x\nux = [5.0, 5]  # upper bounds on x\nng = 2  # number of constraints\nlg = -Inf*one(ng)  # lower bounds on g\nug = zeros(ng)  # upper bounds on g\noptions = Options(derivatives=UserDeriv())\n\nxopt, fopt, info = minimize(deriv, x0, ng, lx, ux, lg, ug, options)","category":"page"},{"location":"guide.html#Derivatives-(Sparse-Jacobians)","page":"Guide","title":"Derivatives (Sparse Jacobians)","text":"","category":"section"},{"location":"guide.html","page":"Guide","title":"Guide","text":"Several methods exist to help detect sparsity patterns.  Given a function and provided bounds on x, the following  method generates three random points within the bounds and computes the Jacobian using either ForwardDiff or finite differencing. Elements of the Jacobian that are zero at all three spots are assumed to always be zero.  The resulting sparsity pattern is returned.  ","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"using SNOW\n\n# an example with a sparse Jacobian\nfunction example(g, x)\n    f = 1.0\n\n    g[1] = x[1]^2 + x[4]\n    g[2] = 3*x[2]\n    g[3] = x[2]*x[4]\n    g[4] = x[1]^3 + x[3]^3 + x[5]\nend\n\nng = 4  # number of constraints\nlx = -5*ones(5)  # lower bounds for x\nux = 5*ones(5)  # upper bounds for x\nsp  = SparsePattern(ForwardAD(), example, ng, lx, ux)","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"or with central differencing","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"sp = SparsePattern(CentralFD(), example, ng, lx, ux)","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"Alternative approach include specifying the three points directly:","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"x1 = rand(5)\nx2 = rand(5)\nx3 = rand(5)\nsp = SparsePattern(ForwardAD(), example, ng, x1, x2, x3)","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"You can also provide a Jacobian directly either in dense or sparse format.","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"sp = SparsePattern(A)  # where A is a Matrix or a SparseMatrixCSC","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"With a provided sparsity pattern, the package can use graph coloring to reduce the number of function calls if applicable, and pass the sparse structure to the optimizer.  The format is similar to the dense case, except you provide two differentiation methods: one for the gradient, and one for the Jacobian (though you could use the same method for both).  Even when the constraint Jacobian is sparse, the function gradient is often dense.  The function gradient in that case is well suited for a reverse mode (if forward mode was used you'd require n_x forward negating the benefits of Jacobian sparsity).","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"options = Options(sparsity=sp, derivatives=[ReverseAD(), ForwardAD()])  # reverse for gradient, sparse forward for Jacobian\nnothing #hide","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"Currently supported options are ReverseAD, or RevZyg for the gradient, and ForwardAD or FD for the Jacobian.","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"You can also provide your own derivatives.  For sparse Jacobians there are a wide variety of possible use cases (structure you can take advantage of, mixed-mode AD, using a combination of analytic and AD, etc.), and so for best performance you may want to provide your own.","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"options = Options(sparsity=sp, derivatives=UserDeriv())\nnothing #hide","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"The function signature is modified like the dense case:","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"f = func!(g, df, dg, x)","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"except dg is a vector (not a matrix) containing the constraint jacobian in the order specified by sp.rows, sp.cols. Like the dense case, the vectors df and dg should be modified in place. ","category":"page"},{"location":"guide.html#Algorithms","page":"Guide","title":"Algorithms","text":"","category":"section"},{"location":"guide.html","page":"Guide","title":"Guide","text":"Currently, you can choose between IPOPT and SNOPT, although the latter required a paid license.  Both methods produce a *.out file defaulting to ipopt.out for the former and snopt-print.out and snopt.summary.out for the latter. The names of the files can be changed through the algorithm-specific options.","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"IPOPT takes an optional argument, a dictionary, where ipopt-specific options can be provided.  See a list of options here.","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"using SNOW\n\nfunction simple!(g, x)\n    # objective\n    f = 4*x[1]^2 - x[1] - x[2] - 2.5\n    \n    # constraints\n    g[1] = -x[2]^2 + 1.5*x[1]^2 - 2*x[1] + 1\n    g[2] = x[2]^2 + 2*x[1]^2 - 2*x[1] - 4.25\n    g[3] = x[1]^2 + x[1]*x[2] - 10.0\n\n    return f\nend\nx0 = [1.0; 2.0]  # starting point\nlx = [-5.0, -5]  # lower bounds on x\nux = [5.0, 5]  # upper bounds on x\nng = 3  # number of constraints\nlg = -Inf*one(ng)  # lower bounds on g\nug = zeros(ng)  # upper bounds on g\n\n# ----- set some options ------\nip_options = Dict(\n    \"max_iter\" => 3,\n    \"tol\" => 1e-6\n)\nsolver = IPOPT(ip_options)\noptions = Options(;solver)  \n\nxopt, fopt, info = minimize(simple!, x0, ng, lx, ux, lg, ug, options)","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"SNOPT has three optional argument: a dictionary of snopt-specific options (see Snopt documentation), a Snopt.Names object to define custom names in the output file (see https://github.com/byuflowlab/Snopt.jl), and a warmstart object (explained below).","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"SNOPT","category":"page"},{"location":"guide.html#SNOW.SNOPT","page":"Guide","title":"SNOW.SNOPT","text":"SNOPT(;options=Dict(), names=Snopt.Names(), warmstart=nothing)\n\nUse Snopt as the optimizer\n\nArguments\n\noptions::Dict: options for Snopt.  see Snopt docs.\nnames::Snopt.Names: custom names for function and variables.\nwarmstart::Snopt.Start: a warmstart object (one of the outputs of Snopt.Outputs)\n\n\n\n\n\n","category":"type"},{"location":"guide.html","page":"Guide","title":"Guide","text":"Snopt also returns a fourth output, which is a struct Snopt.Out containing information like the number of iterations, function calls, solve time, constraint values, and a warm start object.  That warm start object can be put back in as an input for a later run (it contains final values for x, f, Lagrange multipliers, etc.)","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"The below example shows setting options, extracting some outputs, and using a warm start.  Note that Snopt must be loaded separately.","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"using Snopt\nusing SNOW\n\nfunction fun(g, x)\n\n    f = x[1]^2 - x[2]\n\n    g[1] = x[2] - 2*x[1]\n    g[2] = -x[2]\nend\n\nx0 = [10.0; 10.0]\nlx = [0.0; 0.0]\nux = [20.0; 50.0]\nng = 2\nlg = -Inf*ones(ng)\nug = zeros(ng)\n\n# artificially limiting the major iterations so we can restart\nsnopt_opt = Dict(\n    \"Major iterations limit\" => 2\n)\n\nsolver = SNOPT(options=snopt_opt)\noptions = Options(;solver) \n\nxopt, fopt, info, out = minimize(fun, x0, ng, lx, ux, lg, ug, options)\nprintln(\"major iter = \", out.major_iter)\nprintln(\"iterations = \", out.iterations)\nprintln(\"solve time = \", out.run_time)\n\n# warm start from where we stopped\nsolver = SNOPT(warmstart=out.warm)\noptions = Options(;solver) \nxopt, fopt, info, out = minimize(fun, x0, ng, lx, ux, lg, ug, options)\nprintln(\"major iter = \", out.major_iter)\nprintln(\"iterations = \", out.iterations)\nprintln(\"solve time = \", out.run_time)","category":"page"},{"location":"guide.html#Interface","page":"Guide","title":"Interface","text":"","category":"section"},{"location":"guide.html","page":"Guide","title":"Guide","text":"The options are set as follows","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"Options","category":"page"},{"location":"guide.html#SNOW.Options","page":"Guide","title":"SNOW.Options","text":"Options(;sparsity=DensePattern(), derivatives=ForwardFD(), solver=IPOPT())\n\nOptions for SNOW.  Default is dense, forward finite differencing, and IPOPT.\n\nArguments\n\nsparsity::AbstractSparsityPattern: specify sparsity pattern\nderivatives::AbstractDiffMethod: specific differentiation methods to use   or derivatives::Vector{AbstractDiffMethod}: vector of length two,    first for gradient differentiation method, second for jacobian differentiation method\nsolver::AbstractSolver: specificy which optimizer to use\n\n\n\n\n\n","category":"type"},{"location":"guide.html","page":"Guide","title":"Guide","text":"and the main function is minimize:","category":"page"},{"location":"guide.html","page":"Guide","title":"Guide","text":"minimize","category":"page"},{"location":"guide.html#SNOW.minimize","page":"Guide","title":"SNOW.minimize","text":"minimize(func!, x0, ng, lx=-Inf, ux=Inf, lg=-Inf, ug=0.0, options=Options())\n\nsolve the optimization problem\n\ntextminimize quad    f(x)  textsubject to quad  l_g le g(x) le u_g  quadquadquadquad   l_x le x le u_x\n\nf = func!(g, x), unless user-supplied derivatives then: f = func!(g, df, dg, x)\n\nequality constraint for the ith constraint: lg[i] = ug[i]\n\n\n\n\n\n","category":"function"},{"location":"quickstart.html#QuickStart","page":"Quick Start","title":"QuickStart","text":"","category":"section"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"Snopt solves optimization problems of the following form:","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"beginaligned\ntextminimize quad  f(x)  \ntextsubject to quad  l_g le g(x) le u_g  \n    quad l_x le x le u_x\nendaligned","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"Equality constraints can be specified by setting l_g_i = u_g_i.  The expected function signature is:","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"f = func!(g, x)","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"where g is modified in-place.  The optimizer returns x^* f^*, an information message regarding how the algorithm terminated, and potentially a struct with additional solver-specific outputs.","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"We start by loading the package.","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"using SNOW","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"note: Snopt\nIf you want to use Snopt as the optimizer you need to build Snopt.jl separately and load the package separate from this one using Snopt (either before or after using SNOW).  It can't be loaded by default because the code is not freely available.  In this example we use Ipopt for the optimization as it is freely available.","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"Next, we define the function we wish to optimize.  ","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"function simple!(g, x)\n    # objective\n    f = 4*x[1]^2 - x[1] - x[2] - 2.5\n    \n    # constraints\n    g[1] = -x[2]^2 + 1.5*x[1]^2 - 2*x[1] + 1\n    g[2] = x[2]^2 + 2*x[1]^2 - 2*x[1] - 4.25\n    g[3] = x[1]^2 + x[1]*x[2] - 10.0\n\n    return f\nend\nnothing # hide","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"We can now optimize the function as follows:","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"x0 = [1.0; 2.0]  # starting point\nng = 3  # number of constraints\nxopt, fopt, info = minimize(simple!, x0, ng)","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"While the defaults are suitable in this case, for the purposes of demonstration we will be more explicit about the bounds on x (defaults to -Inf to +Inf) and the bounds on the nonlinear constraints (defaults to g(x) <= 0).  We will also explicitly set the IPOPT optimizer (also a default).","category":"page"},{"location":"quickstart.html","page":"Quick Start","title":"Quick Start","text":"x0 = [1.0; 2.0]  # starting point\nlx = [-5.0, -5]  # lower bounds on x\nux = [5.0, 5]  # upper bounds on x\nng = 3  # number of constraints\nlg = -Inf*ones(ng)  # lower bounds on g\nug = zeros(ng)  # upper bounds on g\noptions = Options(solver=IPOPT())  # choosing IPOPT solver\n\nxopt, fopt, info = minimize(simple!, x0, ng, lx, ux, lg, ug, options)\n\nprintln(\"xstar = \", xopt)\nprintln(\"fstar = \", fopt)\nprintln(\"info = \", info)","category":"page"},{"location":"index.html#SNOW","page":"Home","title":"SNOW","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Summary: A wrapper for various sparse nonlinear optimization algorithms and derivative computation packages.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Author: Andrew Ning","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Features:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Allows easy switching between SNOPT and IPOPT from a common interface passing through all solver options, preserving output in files, and allowing warm starts (for SNOPT).\nEasy switching between various differentiation methods: ForwardDiff, ReverseDiff, Zygote, FiniteDiff (forward, central, complex step), and user-defined derivatives.\nDerivative calculations are all non-allocating during optimization.\nOutputs are also cached as applicable to avoid unnecessary function calls.\nMethods are provided to help determine sparsity patterns, sparse Jacobians can be updated efficiently with SparseDiffTools (using graph coloring), and the sparsity structure is passed to the solvers.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Documentation:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Start with the quick start to learn basic usage.\nMore advanced or specific queries are addressed in the guide.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Run Unit Tests:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"pkg> activate .\npkg> test","category":"page"}]
}
